# Arabic EEG Imagined Speech Classification ğŸ§ 

End-to-end deep learning pipeline for Arabic imagined speech decoding from EEG signals.

[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow 2.x](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![CI](https://github.com/HATIMABDESSAMAD/EEG-Arabic-Imagined-Speech-CNN-Transformer/actions/workflows/ci.yml/badge.svg)](https://github.com/HATIMABDESSAMAD/EEG-Arabic-Imagined-Speech-CNN-Transformer/actions/workflows/ci.yml)

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Dataset](#-dataset)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Results](#-results)
- [Project Structure](#-project-structure)
- [EDA Scripts](#-eda-scripts)
- [Contributing](#-contributing)

---

## ğŸ¯ Project Overview

This project implements a **CNN + Transformer** hybrid architecture for classifying **16 Arabic words** from EEG brain signals. The system decodes imagined speech from non-invasive EEG recordings, enabling potential applications in:

- ğŸ¦½ **Assistive technology** for paralyzed patients
- ğŸ—£ï¸ **Silent speech interfaces**
- ğŸ§  **Brain-computer interfaces (BCI)**

### Key Features

- âœ… Multi-band frequency filtering (Theta, Alpha, Beta)
- âœ… Channel attention mechanism (Squeeze-and-Excitation)
- âœ… Transformer encoder for temporal modeling
- âœ… Advanced data augmentation for EEG
- âœ… Complete preprocessing pipeline
- âœ… Pre-trained model included

---

## ğŸ“Š Dataset

**ArEEG_Words** - Arabic Imagined Speech EEG Dataset

| Property | Value |
|----------|-------|
| **Classes** | 16 Arabic words |
| **Recordings** | 359 CSV files |
| **Participants** | 24 subjects |
| **EEG Channels** | 14 (Emotiv EPOC X) |
| **Sampling Rate** | 128 Hz |
| **Duration** | ~11 seconds per recording |

### Word Classes

| Arabic | English | Arabic | English |
|--------|---------|--------|---------|
| Ø§Ø®ØªØ± | Select | Ø­Ù…Ø§Ù… | Bathroom |
| Ø§Ø³ÙÙ„ | Down | Ø¯ÙˆØ§Ø¡ | Medicine |
| Ø§Ø¹Ù„Ù‰ | Up | Ø¹Ø·Ø´ | Thirst |
| Ø§Ù†Ø°Ø§Ø± | Alarm | Ù„Ø§ | No |
| Ø§ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ | Stop | Ù…Ø³Ø§ÙØ© | Space |
| ØªØ´ØºÙŠÙ„ | Start | Ù†Ø¹Ù… | Yes |
| Ø¬ÙˆØ¹ | Hunger | ÙŠØ³Ø§Ø± | Left |
| Ø­Ø°Ù | Delete | ÙŠÙ…ÙŠÙ† | Right |

### EEG Channels

```
AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4
```

---

## ğŸ—ï¸ Architecture

### Deep Learning Pipeline (CNN + Transformer)

```
Raw EEG (14 channels Ã— 128 Hz)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Band Butterworth Filter â”‚
â”‚  â€¢ Theta (4-8 Hz)              â”‚
â”‚  â€¢ Alpha (8-13 Hz)             â”‚
â”‚  â€¢ Beta (13-30 Hz)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    14 Ã— 3 = 42 channels
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Segmentation (1s epochs)    â”‚
â”‚    Overlap: 85%                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Z-score Normalization        â”‚
â”‚   (per channel, no leakage)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CNN Feature Extraction       â”‚
â”‚   â€¢ Conv1D + BatchNorm + GELU  â”‚
â”‚   â€¢ Channel Attention (SE)     â”‚
â”‚   â€¢ Residual connections       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Encoder          â”‚
â”‚   â€¢ 2 layers, 4 heads          â”‚
â”‚   â€¢ Positional encoding        â”‚
â”‚   â€¢ Multi-head attention       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Classification Head          â”‚
â”‚   â€¢ Global Average Pooling     â”‚
â”‚   â€¢ Dense + Dropout            â”‚
â”‚   â€¢ Softmax (16 classes)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Component | Details |
|-----------|---------|
| Input Shape | (128, 42) |
| CNN Filters | 72 |
| Transformer Heads | 4 |
| Transformer Layers | 2 |
| Total Parameters | ~120K |
| Regularization | Dropout (0.25), L2, Label Smoothing |

---

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8+
- NVIDIA GPU (recommended) with CUDA support
- 8GB+ RAM

### Setup

```bash
# Clone the repository
git clone https://github.com/HATIMABDESSAMAD/EEG-Arabic-Imagined-Speech-CNN-Transformer.git
cd EEG-Arabic-Imagined-Speech-CNN-Transformer

# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Verify Installation

```bash
python test_installation.py
```

---

## ğŸš€ Quick Start

### 1. Training

**Full training (recommended):**

```bash
python train_advanced_model.py --data_root ./data --output_dir ./outputs --epochs 200 --overlap 0.85
```

**Quick test (dry run):**

```bash
python train_advanced_model.py --dry_run --epochs 10
```

**Available arguments:**

| Argument | Default | Description |
|----------|---------|-------------|
| `--data_root` | `./data` | Path to dataset |
| `--output_dir` | `./outputs_advanced` | Output directory |
| `--epochs` | 200 | Training epochs |
| `--batch_size` | 32 | Batch size |
| `--overlap` | 0.85 | Epoch overlap (0-1) |
| `--seed` | 42 | Random seed |
| `--dry_run` | False | Test with 1 file/class |

### 2. Prediction

**Single file:**

```bash
python predict.py --model_dir ./outputs_advanced --file ./new_recording.csv
```

**Directory:**

```bash
python predict.py --model_dir ./outputs_advanced --directory ./test_data --output predictions.csv
```

### 3. Visualization

```bash
python visualize_results.py --output_dir ./outputs_advanced
```

---

## ğŸ“ˆ Results

### Performance Metrics

| Metric | Value |
|--------|-------|
| **Test Accuracy** | **85.6%** |
| **F1-Score (macro)** | **85.5%** |
| **Baseline (random)** | 6.25% |
| **Improvement** | **13.7Ã— better than random** |

### Per-Class Performance

| Best Classes | F1-Score | Challenging Classes | F1-Score |
|--------------|----------|---------------------|----------|
| Ø§Ø³ÙÙ„ (Down) | 93.4% | Ø§Ø¹Ù„Ù‰ (Up) | 84.2% |
| Ø§Ù†Ø°Ø§Ø± (Alarm) | 90.9% | ÙŠØ³Ø§Ø± (Left) | 82.1% |
| Ø§Ø®ØªØ± (Select) | 90.7% | Ø§ÙŠÙ‚Ø§Ù ØªØ´ØºÙŠÙ„ (Stop) | 80.5% |

### Confusion Matrix

![Confusion Matrix](outputs_advanced/confusion_matrix.png)

---

## ğŸ“ Project Structure

```
arabic-eeg-speech/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # Documentation
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md              # Contribution guidelines
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ§  train_advanced_model.py      # Main training script
â”œâ”€â”€ ğŸ”® predict.py                   # Inference script
â”œâ”€â”€ ğŸ“Š visualize_results.py         # Plot results
â”œâ”€â”€ ğŸ§ª test_installation.py         # Verify setup
â”‚
â”œâ”€â”€ ğŸ“ .github/workflows/           # CI/CD pipelines
â”‚   â””â”€â”€ ci.yml                      # GitHub Actions workflow
â”‚
â”œâ”€â”€ ğŸ“ outputs_advanced/            # Trained model & results
â”‚   â”œâ”€â”€ best_model.keras            # Trained model weights
â”‚   â”œâ”€â”€ normalization_stats.npz     # Preprocessing stats
â”‚   â”œâ”€â”€ test_metrics.json           # Performance metrics
â”‚   â”œâ”€â”€ training_history.json       # Training curves data
â”‚   â””â”€â”€ confusion_matrix.png        # Confusion matrix plot
â”‚
â”œâ”€â”€ ğŸ“ eda/                         # EDA & Traditional ML pipeline
â”‚   â”œâ”€â”€ main.py                     # Traditional ML pipeline
â”‚   â”œâ”€â”€ eda_areeg_words.py          # Exploratory data analysis
â”‚   â”œâ”€â”€ csp_ovr.py                  # Common Spatial Patterns
â”‚   â”œâ”€â”€ nca_selection.py            # Feature selection (NCA)
â”‚   â”œâ”€â”€ stacking_model.py           # Ensemble classifier
â”‚   â””â”€â”€ config.py                   # Configuration
â”‚
â””â”€â”€ ğŸ“ data/                        # Dataset (16 classes)
    â”œâ”€â”€ select/                     # Ø§Ø®ØªØ± - Select
    â”œâ”€â”€ down/                       # Ø§Ø³ÙÙ„ - Down
    â”œâ”€â”€ up/                         # Ø§Ø¹Ù„Ù‰ - Up
    â”œâ”€â”€ ...                         # (13 more classes)
    â””â”€â”€ eda_*.png/csv               # EDA visualizations
```

---

## ğŸ”¬ EDA Scripts

The `eda/` folder contains **exploratory data analysis** and an alternative **traditional machine learning** pipeline:

### Scripts

| Script | Description |
|--------|-------------|
| `eda_areeg_words.py` | Exploratory data analysis & visualizations |
| `main.py` | Full ML pipeline (CSP + Stacking ensemble) |
| `csp_ovr.py` | Common Spatial Patterns (One-vs-Rest) |
| `nca_selection.py` | Neighborhood Component Analysis |
| `stacking_model.py` | Ensemble classifier (LDA, SVM, RF, KNN) |
| `preprocess.py` | Signal preprocessing utilities |

### Run EDA Pipeline

```bash
cd eda
python eda_areeg_words.py --data_dir "../data"
```

### Run Traditional ML Pipeline

```bash
cd eda
python main.py --data_dir "../data" --output_dir "./output" --n_splits 5
```

### EDA Results (in `data/` folder)

| File | Description |
|------|-------------|
| `eda_distribution_fichiers.png` | File distribution per class |
| `eda_analyse_signaux_eeg.png` | EEG signal analysis |
| `eda_analyse_par_mot.png` | Per-word analysis |
| `eda_qualite_signal.png` | Signal quality metrics |
| `eda_statistiques_eeg.csv` | EEG statistics |
| `eda_features_par_mot.csv` | Features per word class |

---

## ğŸ”§ Technical Details

### Preprocessing

1. **Multi-band filtering**: Butterworth bandpass (order=4)
   - Theta (4-8 Hz): Memory and learning
   - Alpha (8-13 Hz): Relaxation and attention
   - Beta (13-30 Hz): Cognitive activity

2. **Segmentation**: 1-second windows with 85% overlap

3. **Normalization**: Z-score per channel (fitted on training set only)

### Data Augmentation

- Gaussian noise injection (Ïƒ=0.02)
- Temporal shifting (Â±8 samples)
- Channel dropout (10%)
- Amplitude scaling (Â±10%)

### Training Strategy

- **Optimizer**: AdamW with cosine annealing
- **Loss**: Categorical cross-entropy with label smoothing (0.1)
- **Early stopping**: patience=20 epochs
- **Learning rate**: 1e-3 â†’ 1e-5

---

## ğŸ“š References

- ArEEG Dataset: Arabic Imagined Speech EEG
- EPOC X: Emotiv 14-channel EEG headset
- Architecture inspired by EEGNet and Conformer

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ™ Acknowledgments

- ArEEG Dataset: Arabic Imagined Speech EEG
- Emotiv EPOC X: 14-channel EEG headset
- Architecture inspired by EEGNet and Conformer

---

**â­ If you find this project useful, please consider giving it a star!**
